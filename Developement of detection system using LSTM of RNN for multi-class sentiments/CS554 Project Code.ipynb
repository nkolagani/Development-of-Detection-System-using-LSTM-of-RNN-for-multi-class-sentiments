{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "986ce81647d2c1a554e91d05692d30b593a144a4"
   },
   "source": [
    "## Importing dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test.tsv', 'sampleSubmission.csv', 'train.tsv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import unicodedata, re, string\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "print(os.listdir(\"../input\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3ea4268a41e63dbf967bdaf1d8a66112e0e295de"
   },
   "source": [
    "## Exploring The  Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "81953f0026a9fd8fc234e76b9dc225335efbc354"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../input/train.tsv\", sep=\"\\t\")\n",
    "df_test = pd.read_csv(\"../input/test.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "f69cc2ba0f2cb2ab79807a16b4d252b7a4339d6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 156060 entries, 0 to 156059\n",
      "Data columns (total 4 columns):\n",
      "PhraseId      156060 non-null int64\n",
      "SentenceId    156060 non-null int64\n",
      "Phrase        156060 non-null object\n",
      "Sentiment     156060 non-null int64\n",
      "dtypes: int64(3), object(1)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "109a0161dc81056c529f6f10cb558fd4e06a8206"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PhraseId  SentenceId                                             Phrase  \\\n",
       "0         1           1  A series of escapades demonstrating the adage ...   \n",
       "1         2           1  A series of escapades demonstrating the adage ...   \n",
       "2         3           1                                           A series   \n",
       "3         4           1                                                  A   \n",
       "4         5           1                                             series   \n",
       "\n",
       "   Sentiment  \n",
       "0          1  \n",
       "1          2  \n",
       "2          2  \n",
       "3          2  \n",
       "4          2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "30d1ae62f1dc4f3decfd1fea4eca717c2c1ff288"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A series of escapades demonstrating the adage that what is good for the goose is also good for the gander , some of which occasionally amuses but none of which amounts to much of a story .'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['Phrase'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "bad32939bedaef058d0ecb0a0fae2ec5adee250e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PhraseId</th>\n",
       "      <th>SentenceId</th>\n",
       "      <th>Phrase</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>A series of escapades demonstrating the adage ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>A series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>series</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>of escapades demonstrating the adage that what...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades demonstrating the adage that what is...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>escapades</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating the adage that what is good for ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating the adage</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>demonstrating</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>the adage</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>adage</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>that what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>that</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>what is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>what</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>is good for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>is</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>good for the goose</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>good</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>for the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>for</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>the goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>goose</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>is also good for the gander , some of which oc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>is also good for the gander , some of which oc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>is also</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>the gander , some of which occasionally amuses...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>the gander ,</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>the gander</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>gander</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>,</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>some of which occasionally amuses but none of ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>some of which</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>some</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>of which</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "      <td>which</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>1</td>\n",
       "      <td>occasionally amuses but none of which amounts ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>occasionally</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>amuses but none of which amounts to much of a ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>amuses</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>48</td>\n",
       "      <td>1</td>\n",
       "      <td>but none of which amounts to much of a story</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>but</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "      <td>none of which amounts to much of a story</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>of which amounts to much of a story</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>which amounts to much of a story</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>54</td>\n",
       "      <td>1</td>\n",
       "      <td>amounts to much of a story</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>amounts</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>to much of a story</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>to</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>58</td>\n",
       "      <td>1</td>\n",
       "      <td>much of a story</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>59</td>\n",
       "      <td>1</td>\n",
       "      <td>much</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>60</td>\n",
       "      <td>1</td>\n",
       "      <td>of a story</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>61</td>\n",
       "      <td>1</td>\n",
       "      <td>a story</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>story</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    PhraseId  SentenceId                                             Phrase  \\\n",
       "0          1           1  A series of escapades demonstrating the adage ...   \n",
       "1          2           1  A series of escapades demonstrating the adage ...   \n",
       "2          3           1                                           A series   \n",
       "3          4           1                                                  A   \n",
       "4          5           1                                             series   \n",
       "5          6           1  of escapades demonstrating the adage that what...   \n",
       "6          7           1                                                 of   \n",
       "7          8           1  escapades demonstrating the adage that what is...   \n",
       "8          9           1                                          escapades   \n",
       "9         10           1  demonstrating the adage that what is good for ...   \n",
       "10        11           1                            demonstrating the adage   \n",
       "11        12           1                                      demonstrating   \n",
       "12        13           1                                          the adage   \n",
       "13        14           1                                                the   \n",
       "14        15           1                                              adage   \n",
       "15        16           1                    that what is good for the goose   \n",
       "16        17           1                                               that   \n",
       "17        18           1                         what is good for the goose   \n",
       "18        19           1                                               what   \n",
       "19        20           1                              is good for the goose   \n",
       "20        21           1                                                 is   \n",
       "21        22           1                                 good for the goose   \n",
       "22        23           1                                               good   \n",
       "23        24           1                                      for the goose   \n",
       "24        25           1                                                for   \n",
       "25        26           1                                          the goose   \n",
       "26        27           1                                              goose   \n",
       "27        28           1  is also good for the gander , some of which oc...   \n",
       "28        29           1  is also good for the gander , some of which oc...   \n",
       "29        30           1                                            is also   \n",
       "..       ...         ...                                                ...   \n",
       "33        34           1  the gander , some of which occasionally amuses...   \n",
       "34        35           1                                       the gander ,   \n",
       "35        36           1                                         the gander   \n",
       "36        37           1                                             gander   \n",
       "37        38           1                                                  ,   \n",
       "38        39           1  some of which occasionally amuses but none of ...   \n",
       "39        40           1                                      some of which   \n",
       "40        41           1                                               some   \n",
       "41        42           1                                           of which   \n",
       "42        43           1                                              which   \n",
       "43        44           1  occasionally amuses but none of which amounts ...   \n",
       "44        45           1                                       occasionally   \n",
       "45        46           1  amuses but none of which amounts to much of a ...   \n",
       "46        47           1                                             amuses   \n",
       "47        48           1       but none of which amounts to much of a story   \n",
       "48        49           1                                                but   \n",
       "49        50           1           none of which amounts to much of a story   \n",
       "50        51           1                                               none   \n",
       "51        52           1                of which amounts to much of a story   \n",
       "52        53           1                   which amounts to much of a story   \n",
       "53        54           1                         amounts to much of a story   \n",
       "54        55           1                                            amounts   \n",
       "55        56           1                                 to much of a story   \n",
       "56        57           1                                                 to   \n",
       "57        58           1                                    much of a story   \n",
       "58        59           1                                               much   \n",
       "59        60           1                                         of a story   \n",
       "60        61           1                                            a story   \n",
       "61        62           1                                              story   \n",
       "62        63           1                                                  .   \n",
       "\n",
       "    Sentiment  \n",
       "0           1  \n",
       "1           2  \n",
       "2           2  \n",
       "3           2  \n",
       "4           2  \n",
       "5           2  \n",
       "6           2  \n",
       "7           2  \n",
       "8           2  \n",
       "9           2  \n",
       "10          2  \n",
       "11          2  \n",
       "12          2  \n",
       "13          2  \n",
       "14          2  \n",
       "15          2  \n",
       "16          2  \n",
       "17          2  \n",
       "18          2  \n",
       "19          2  \n",
       "20          2  \n",
       "21          3  \n",
       "22          3  \n",
       "23          2  \n",
       "24          2  \n",
       "25          2  \n",
       "26          2  \n",
       "27          2  \n",
       "28          2  \n",
       "29          2  \n",
       "..        ...  \n",
       "33          1  \n",
       "34          2  \n",
       "35          2  \n",
       "36          2  \n",
       "37          2  \n",
       "38          2  \n",
       "39          2  \n",
       "40          2  \n",
       "41          2  \n",
       "42          2  \n",
       "43          2  \n",
       "44          2  \n",
       "45          2  \n",
       "46          3  \n",
       "47          1  \n",
       "48          2  \n",
       "49          1  \n",
       "50          2  \n",
       "51          2  \n",
       "52          2  \n",
       "53          2  \n",
       "54          2  \n",
       "55          2  \n",
       "56          2  \n",
       "57          2  \n",
       "58          2  \n",
       "59          2  \n",
       "60          2  \n",
       "61          2  \n",
       "62          2  \n",
       "\n",
       "[63 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.loc[df_train['SentenceId'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d6b71ead238ca90002183a854334da49dd319360"
   },
   "source": [
    "As was mentioned in the original competition description, there are no individual movie reviews but rather phrases taken out of context and split into smaller parts, each with an assigned sentiment category. The competition is evaluated based on scoring results of each test phrase, so the context of the whole review does not matter here. The data is also fairly clean, so there will not be need for much pre-processing.\n",
    "Before proceeding it is also a good idea to look at distribution of data, to see if the classes in training set are evenly distributed. For that I borrowed code from another Kaggle kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "14a584929ac8ef23a3bdf184d8fb1f65556fac38"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dist = df_train.groupby([\"Sentiment\"]).size()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "sns.barplot(dist.keys(), dist.values);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3485562c7cd2ac11917b5e9953b761ad94110a2e"
   },
   "source": [
    "Classes seem to follow a normal distribution, with most frequently distributed class being \"2\".  This could lead to model not having sufficient data to learn the less-represented classes. This is something to be aware of when evaluating the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5ff180b541a1e811cec7086397097281bca17b10"
   },
   "source": [
    "## Pre-Processing\n",
    "\n",
    "Words need to be tokenized into numeric format to be passed to RNN. Before that, however, I will also filter out spaces and punctuation, and use lemmatization to further reduce dimensionality. At this moment I do not want to filter out \"stop-words\", as RNN's are good at learning context from previously encountered information. In case of movie reviews, phrase \"this movie is shit\" has opposite meaning of \"this movie is the shit\", so I want that information to be available to the model.\n",
    "\n",
    "Below are some helper functions I borrowed online to help prepare the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "271abb29213dabf44a4ac92038346dd00f2b83ec"
   },
   "outputs": [],
   "source": [
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_numbers(words):\n",
    "    \"\"\"Remove all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(\"\\d+\", \"\", word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = remove_numbers(words)\n",
    "#    words = remove_stopwords(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c62b3a073970c68147125e5dc4761fab0b111413"
   },
   "source": [
    "Time to get the hands dirty. First I will go through the dataframe and tokenize each word using NLTK. Then I will pass each token through the prepping functions I created earlier, with the end result being a reduced list of lemmatized word tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "0c7e2fa70145639438d6a17c0af8e8c61517768f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [a, series, of, escapades, demonstrating, the,...\n",
       "1    [a, series, of, escapades, demonstrating, the,...\n",
       "2                                          [a, series]\n",
       "3                                                  [a]\n",
       "4                                             [series]\n",
       "Name: Words, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First step - tokenizing phrases\n",
    "df_train['Words'] = df_train['Phrase'].apply(nltk.word_tokenize)\n",
    "\n",
    "# Second step - passing through prep functions\n",
    "df_train['Words'] = df_train['Words'].apply(normalize) \n",
    "df_train['Words'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "157930b980a9f30179276767d9b44bdef8c76af8"
   },
   "source": [
    "Looks ok. Now the next prep step - converting words to number representations, as the embedding lookup requires that integers are passed to the network. The easiest way to do this is to create dictionaries that map the words in the vocabulary to integers. Using this vocab then words in each phrase can be converted to integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "d6ed075962d2b8cabb2193803d7cd5e8ea7b8ca4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16209\n",
      "16209\n"
     ]
    }
   ],
   "source": [
    "# Third step - creating a list of unique words to be used as dictionary for encoding\n",
    "word_set = set()\n",
    "for l in df_train['Words']:\n",
    "    for e in l:\n",
    "        word_set.add(e)\n",
    "        \n",
    "word_to_int = {word: ii for ii, word in enumerate(word_set, 1)}\n",
    "\n",
    "# Check if they are still the same lenght\n",
    "print(len(word_set))\n",
    "print(len(word_to_int))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "7a41485c79f55a5981b07b77abc67cf2e2046ee7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [4734, 7706, 1467, 11411, 6263, 15150, 3271, 1...\n",
       "1    [4734, 7706, 1467, 11411, 6263, 15150, 3271, 1...\n",
       "2                                         [4734, 7706]\n",
       "3                                               [4734]\n",
       "4                                               [7706]\n",
       "Name: Tokens, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now the dict to tokenize each phrase\n",
    "df_train['Tokens'] = df_train['Words'].apply(lambda l: [word_to_int[word] for word in l])\n",
    "df_train['Tokens'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "40ce0f1b2671f9b542f55b634a80996d7d80ebed"
   },
   "source": [
    "So far so good. But for the input to the network the length of each phrase sequence has to be equal, so the shorter phrases will need to be \"padded\" - zeros added so that their token numbers are the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "bd4c04ded25ff052cff54f3a3695d7e880229dca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "# Step four - get the len of longest phrase\n",
    "max_len = df_train['Tokens'].str.len().max()\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "380adf993084f5aec052f0816be116680f44e403",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4734  7706  1467 11411  6263 15150  3271 10684  7577 14821 14459 15578\n",
      "  15150  4062 14821 11072 14459 15578 15150  1584  8704  1467  9235 13130\n",
      "  10904 13862 11013  1467  9235 12041  8248 11118  1467  4734  7777     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
      " [ 4734  7706  1467 11411  6263 15150  3271 10684  7577 14821 14459 15578\n",
      "  15150  4062     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0]\n",
      " [ 4734  7706     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "# Pad each phrase representation with zeroes, starting from the beginning of sequence\n",
    "# Will use a combined list of phrases as np array for further work. This is expected format for the Pytorch utils to be used later\n",
    "\n",
    "all_tokens = np.array([t for t in df_train['Tokens']])\n",
    "encoded_labels = np.array([l for l in df_train['Sentiment']])\n",
    "\n",
    "# Create blank rows\n",
    "features = np.zeros((len(all_tokens), max_len), dtype=int)\n",
    "# for each phrase, add zeros at the end \n",
    "for i, row in enumerate(all_tokens):\n",
    "    features[i, :len(row)] = row\n",
    "\n",
    "#print first 3 values of the feature matrix \n",
    "print(features[:3])\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "513a47fbb21155a850e0845839aec12962afe5f8"
   },
   "source": [
    "## Splitting the Data for Training, Validation, Test\n",
    "\n",
    "Time to split the data into training, validation, and test sets. For this purpose I will reserve 80% of training data for training, and remaining 20% will be split equally for validation and testing purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "9879d59cdaba3f64bafe7e1724251ee80dc3a16a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\tFeature Shapes:\n",
      "Train set: \t\t(124848, 48) \n",
      "Validation set: \t(15606, 48) \n",
      "Test set: \t\t(15606, 48)\n"
     ]
    }
   ],
   "source": [
    "split_frac = 0.8\n",
    "\n",
    "## split data into training, validation, and test data (features and labels, x and y)\n",
    "\n",
    "split_idx = int(len(features)*0.8)\n",
    "train_x, remaining_x = features[:split_idx], features[split_idx:]\n",
    "train_y, remaining_y = encoded_labels[:split_idx], encoded_labels[split_idx:]\n",
    "\n",
    "test_idx = int(len(remaining_x)*0.5)\n",
    "val_x, test_x = remaining_x[:test_idx], remaining_x[test_idx:]\n",
    "val_y, test_y = remaining_y[:test_idx], remaining_y[test_idx:]\n",
    "\n",
    "## print out the shapes of  resultant feature data\n",
    "print(\"\\t\\t\\tFeature Shapes:\")\n",
    "print(\"Train set: \\t\\t{}\".format(train_x.shape), \n",
    "      \"\\nValidation set: \\t{}\".format(val_x.shape),\n",
    "      \"\\nTest set: \\t\\t{}\".format(test_x.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "77cd3cba1b1a3546fe3f6903399f399ef217884a"
   },
   "source": [
    "## DataLoaders and Batching\n",
    "After creating training, test, and validation data, time top create DataLoaders. They are the expected way to pass data into the model for training / testing. Loaders are created by following two steps:\n",
    "\n",
    "1) Create a known format for accessing data, using TensorDataset which takes in an input set of data and a target set of data with the same first dimension, and creates a dataset.\n",
    "\n",
    "2) Create DataLoaders and batch our training, validation, and test Tensor datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "1f71a409ddc465da2108550c6260120b11db5a7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2312\n",
      "289\n",
      "289\n"
     ]
    }
   ],
   "source": [
    "# create Tensor datasets\n",
    "train_data = TensorDataset(torch.from_numpy(train_x), torch.from_numpy(train_y))\n",
    "valid_data = TensorDataset(torch.from_numpy(val_x), torch.from_numpy(val_y))\n",
    "test_data = TensorDataset(torch.from_numpy(test_x), torch.from_numpy(test_y))\n",
    "\n",
    "# dataloaders\n",
    "batch_size = 54\n",
    "\n",
    "# make sure the SHUFFLE your training data\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "valid_loader = DataLoader(valid_data, shuffle=True, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "# Check the size of the loaders (how many batches inside)\n",
    "print(len(train_loader))\n",
    "print(len(valid_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d3621e913b85c6015d00b12b11adf5e0f68933a1"
   },
   "source": [
    "## Creating a Deep Network\n",
    "\n",
    "The following text is borrowed from another excersize file, but it describes the approach of model building very well:\n",
    "\n",
    "1) First, we'll pass in words to an embedding layer. We need an embedding layer because we have thousands of words, so we'll need a more efficient representation for our input data than one-hot encoded vectors. In this case, the embedding layer is for dimensionality reduction, rather than for learning semantic representations.\n",
    "\n",
    "2) After input words are passed to an embedding layer, the new embeddings will be passed to LSTM cells. The LSTM cells will add recurrent connections to the network and give us the ability to include information about the sequence of words in the movie review data.\n",
    "LSTM takes in an input_size, a hidden_dim, a number of layers, a dropout probability (for dropout between multiple layers), and a batch_first parameter.\n",
    "\n",
    "3) Finally, the LSTM outputs will go to a linear layer for final classification, which outputs in turn will be passed to cross-entropy loss function to obtain probabilities for each predicted class.\n",
    "\n",
    "The layers are as follows:\n",
    "* An embedding layer that converts word tokens (integers) into embeddings of a specific size.\n",
    "* An LSTM layer defined by a hidden_state size and number of layers\n",
    "* A fully-connected output layer that maps the LSTM layer outputs to a desired output_size\n",
    "* A softmax will be applyed later by Crossentropy loss function, which turns all outputs into a probability\n",
    "\n",
    "Most of the time, the network will have better performance with more layers; between 2-3. Adding more layers allows the network to learn really complex relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "4395f5c0690c68ea73bb0f0d8f78b453c95c49cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU.\n"
     ]
    }
   ],
   "source": [
    "# First checking if GPU is available\n",
    "train_on_gpu=torch.cuda.is_available()\n",
    "\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU.')\n",
    "else:\n",
    "    print('No GPU available, training on CPU.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "083cc4a1a4ea66e1c0188338b53a3f1ddecd1e24"
   },
   "outputs": [],
   "source": [
    "class SentimentRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    The RNN model that will be used to perform Sentiment analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the model by setting up the layers.\n",
    "        \"\"\"\n",
    "        super(SentimentRNN, self).__init__()\n",
    "\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # embedding and LSTM layers\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "        # linear\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \"\"\"\n",
    "        Perform a forward pass of our model on some input and hidden state.\n",
    "        \"\"\"\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # embeddings and lstm_out\n",
    "        embeds = self.embedding(x)\n",
    "\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "\n",
    "        # transform lstm output to input size of linear layers\n",
    "        lstm_out = lstm_out.transpose(0,1)\n",
    "        lstm_out = lstm_out[-1]\n",
    "\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc(out)        \n",
    "\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x hidden_dim,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6b3833c651a26de8c38d005a3566340e43d2a47b"
   },
   "source": [
    "## Instantiate the network\n",
    "Here, we'll instantiate the network. First up, defining the hyperparameters.\n",
    "* vocab_size: Size of our vocabulary or the range of values for our input, word tokens.\n",
    "* output_size: Size of our desired output; the number of class scores we want to output (0..4).\n",
    "* embedding_dim: Number of columns in the embedding lookup table; size of our embeddings.\n",
    "* hidden_dim: Number of units in the hidden layers of our LSTM cells. Usually larger is better performance wise. Common values are 128, 256, 512, etc.\n",
    "* n_layers: Number of LSTM layers in the network. Typically between 1-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "3e1ebcf1dcdf3ae074696b41538b4f593bfa525d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentRNN(\n",
      "  (embedding): Embedding(16210, 400)\n",
      "  (lstm): LSTM(400, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=256, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model w/ hyperparams\n",
    "vocab_size = len(word_to_int)+1 # +1 for the 0 padding\n",
    "output_size = 5\n",
    "embedding_dim = 400\n",
    "hidden_dim = 256\n",
    "n_layers = 2\n",
    "\n",
    "net = SentimentRNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "024b493c908d9323330a1824369b2f9339372c33"
   },
   "source": [
    "## Training Routine\n",
    "Below is the typical training code. \n",
    "Crossentropy loss will be used, since this is multi-class classification problem.\n",
    "We also have some data and training hyparameters:\n",
    "* lr: Learning rate for our optimizer.\n",
    "* epochs: Number of times to iterate through the training dataset.\n",
    "* clip: The maximum gradient value to clip at (to prevent exploding gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "19f913418591679cf69d24df7b7e06b86b56a4be"
   },
   "outputs": [],
   "source": [
    "# loss and optimization functions\n",
    "lr=0.003\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "97778b9a79c598347e9f73712b50323d03405fef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3... Step: 100... Loss: 1.230594... Val Loss: 1.313183\n",
      "Epoch: 1/3... Step: 200... Loss: 1.301816... Val Loss: 1.302561\n",
      "Epoch: 1/3... Step: 300... Loss: 1.377407... Val Loss: 1.302779\n",
      "Epoch: 1/3... Step: 400... Loss: 1.130379... Val Loss: 1.301292\n",
      "Epoch: 1/3... Step: 500... Loss: 1.047430... Val Loss: 1.285534\n",
      "Epoch: 1/3... Step: 600... Loss: 1.207994... Val Loss: 1.255173\n",
      "Epoch: 1/3... Step: 700... Loss: 1.187926... Val Loss: 1.247271\n",
      "Epoch: 1/3... Step: 800... Loss: 1.011234... Val Loss: 1.222823\n",
      "Epoch: 1/3... Step: 900... Loss: 1.128768... Val Loss: 1.216487\n",
      "Epoch: 1/3... Step: 1000... Loss: 1.293388... Val Loss: 1.202958\n",
      "Epoch: 1/3... Step: 1100... Loss: 1.134912... Val Loss: 1.203455\n",
      "Epoch: 1/3... Step: 1200... Loss: 1.018864... Val Loss: 1.188047\n",
      "Epoch: 1/3... Step: 1300... Loss: 1.043085... Val Loss: 1.177926\n",
      "Epoch: 1/3... Step: 1400... Loss: 1.097577... Val Loss: 1.168784\n",
      "Epoch: 1/3... Step: 1500... Loss: 1.299792... Val Loss: 1.158775\n",
      "Epoch: 1/3... Step: 1600... Loss: 1.015123... Val Loss: 1.155795\n",
      "Epoch: 1/3... Step: 1700... Loss: 1.064069... Val Loss: 1.141191\n",
      "Epoch: 1/3... Step: 1800... Loss: 0.991353... Val Loss: 1.128935\n",
      "Epoch: 1/3... Step: 1900... Loss: 0.899944... Val Loss: 1.123493\n",
      "Epoch: 1/3... Step: 2000... Loss: 1.183340... Val Loss: 1.132113\n",
      "Epoch: 1/3... Step: 2100... Loss: 0.943903... Val Loss: 1.116113\n",
      "Epoch: 1/3... Step: 2200... Loss: 1.128480... Val Loss: 1.101652\n",
      "Epoch: 1/3... Step: 2300... Loss: 1.032507... Val Loss: 1.093969\n",
      "Epoch: 2/3... Step: 2400... Loss: 1.034031... Val Loss: 1.133569\n",
      "Epoch: 2/3... Step: 2500... Loss: 0.891838... Val Loss: 1.094414\n",
      "Epoch: 2/3... Step: 2600... Loss: 0.940236... Val Loss: 1.074655\n",
      "Epoch: 2/3... Step: 2700... Loss: 0.635359... Val Loss: 1.091837\n",
      "Epoch: 2/3... Step: 2800... Loss: 0.852405... Val Loss: 1.098328\n",
      "Epoch: 2/3... Step: 2900... Loss: 1.005544... Val Loss: 1.080728\n",
      "Epoch: 2/3... Step: 3000... Loss: 1.123330... Val Loss: 1.061998\n",
      "Epoch: 2/3... Step: 3100... Loss: 1.014239... Val Loss: 1.091719\n",
      "Epoch: 2/3... Step: 3200... Loss: 0.967474... Val Loss: 1.072685\n",
      "Epoch: 2/3... Step: 3300... Loss: 0.746148... Val Loss: 1.078690\n",
      "Epoch: 2/3... Step: 3400... Loss: 0.902277... Val Loss: 1.089117\n",
      "Epoch: 2/3... Step: 3500... Loss: 0.954641... Val Loss: 1.109581\n",
      "Epoch: 2/3... Step: 3600... Loss: 0.930457... Val Loss: 1.051357\n",
      "Epoch: 2/3... Step: 3700... Loss: 1.151211... Val Loss: 1.070662\n",
      "Epoch: 2/3... Step: 3800... Loss: 0.943484... Val Loss: 1.061786\n",
      "Epoch: 2/3... Step: 3900... Loss: 0.746971... Val Loss: 1.042393\n",
      "Epoch: 2/3... Step: 4000... Loss: 0.678907... Val Loss: 1.056320\n",
      "Epoch: 2/3... Step: 4100... Loss: 0.877670... Val Loss: 1.067612\n",
      "Epoch: 2/3... Step: 4200... Loss: 0.893263... Val Loss: 1.041417\n",
      "Epoch: 2/3... Step: 4300... Loss: 0.876042... Val Loss: 1.057672\n",
      "Epoch: 2/3... Step: 4400... Loss: 0.867177... Val Loss: 1.042877\n",
      "Epoch: 2/3... Step: 4500... Loss: 0.931151... Val Loss: 1.050992\n",
      "Epoch: 2/3... Step: 4600... Loss: 1.129846... Val Loss: 1.048043\n",
      "Epoch: 3/3... Step: 4700... Loss: 0.965506... Val Loss: 1.072079\n",
      "Epoch: 3/3... Step: 4800... Loss: 0.747129... Val Loss: 1.059246\n",
      "Epoch: 3/3... Step: 4900... Loss: 0.775600... Val Loss: 1.046859\n",
      "Epoch: 3/3... Step: 5000... Loss: 0.798516... Val Loss: 1.067361\n",
      "Epoch: 3/3... Step: 5100... Loss: 0.725256... Val Loss: 1.036504\n",
      "Epoch: 3/3... Step: 5200... Loss: 0.821798... Val Loss: 1.069153\n",
      "Epoch: 3/3... Step: 5300... Loss: 0.779751... Val Loss: 1.095124\n",
      "Epoch: 3/3... Step: 5400... Loss: 0.696164... Val Loss: 1.087757\n",
      "Epoch: 3/3... Step: 5500... Loss: 1.019551... Val Loss: 1.073094\n",
      "Epoch: 3/3... Step: 5600... Loss: 0.866964... Val Loss: 1.061873\n",
      "Epoch: 3/3... Step: 5700... Loss: 0.883942... Val Loss: 1.073412\n",
      "Epoch: 3/3... Step: 5800... Loss: 0.843106... Val Loss: 1.076148\n",
      "Epoch: 3/3... Step: 5900... Loss: 0.879242... Val Loss: 1.094193\n",
      "Epoch: 3/3... Step: 6000... Loss: 0.961542... Val Loss: 1.052229\n",
      "Epoch: 3/3... Step: 6100... Loss: 0.627484... Val Loss: 1.088085\n",
      "Epoch: 3/3... Step: 6200... Loss: 0.985584... Val Loss: 1.055660\n",
      "Epoch: 3/3... Step: 6300... Loss: 0.907930... Val Loss: 1.052060\n",
      "Epoch: 3/3... Step: 6400... Loss: 1.017026... Val Loss: 1.068408\n",
      "Epoch: 3/3... Step: 6500... Loss: 0.739322... Val Loss: 1.036791\n",
      "Epoch: 3/3... Step: 6600... Loss: 0.904960... Val Loss: 1.058085\n",
      "Epoch: 3/3... Step: 6700... Loss: 0.845617... Val Loss: 1.061320\n",
      "Epoch: 3/3... Step: 6800... Loss: 0.911666... Val Loss: 1.054715\n",
      "Epoch: 3/3... Step: 6900... Loss: 0.622136... Val Loss: 1.095161\n"
     ]
    }
   ],
   "source": [
    "# training params\n",
    "epochs = 3 # 3-4 is approx where I noticed the validation loss stop decreasing\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip=5 # gradient clipping\n",
    "\n",
    "# move model to GPU, if available\n",
    "if(train_on_gpu):\n",
    "    net.cuda()\n",
    "\n",
    "net.train()\n",
    "# train for some number of epochs\n",
    "for e in range(epochs):\n",
    "    # initialize hidden state\n",
    "    h = net.init_hidden(batch_size)\n",
    "\n",
    "    # batch loop\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "\n",
    "        if(train_on_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "        # Creating new variables for the hidden state, otherwise\n",
    "        # we'd backprop through the entire training history\n",
    "        h = tuple([each.data for each in h])\n",
    "\n",
    "        # zero accumulated gradients\n",
    "        net.zero_grad()\n",
    "\n",
    "        # get the output from the model\n",
    "        output, h = net(inputs, h)\n",
    "        # calculate the loss and perform backprop\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "        nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        # loss stats\n",
    "        if counter % print_every == 0:\n",
    "            # Get validation loss\n",
    "            val_h = net.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            net.eval()\n",
    "            for inputs, labels in valid_loader:\n",
    "\n",
    "                # Creating new variables for the hidden state, otherwise\n",
    "                # we'd backprop through the entire training history\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "\n",
    "                if(train_on_gpu):\n",
    "                    inputs, labels = inputs.cuda(), labels.cuda()\n",
    "\n",
    "                output, val_h = net(inputs, val_h)\n",
    "                val_loss = criterion(output, labels)\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            net.train()\n",
    "            print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "43750e21ea76ee1b90c0d3cffed952f68495e510"
   },
   "source": [
    "## Testing\n",
    "\n",
    "There are a few ways to test your network.\n",
    "* Test data performance: First, we'll see how our trained model performs on all of our defined test_data, above. We'll calculate the average loss and accuracy over the test data.\n",
    "* Inference on user-generated data: Second, we'll see if we can input just one example review at a time (without a label), and see what the trained model predicts. Looking at new, user input data like this, and predicting an output label, is called inference.\n",
    "\n",
    "For the practical purposes of this example, though, second option is not applicable, as the task is to classify syntetic colection of provided phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "73615beffd56464165901f7589bacc2dafa0d920"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 1.083\n",
      "Test accuracy: 0.573\n"
     ]
    }
   ],
   "source": [
    "# Get test data loss and accuracy\n",
    "\n",
    "test_losses = [] # track loss\n",
    "num_correct = 0\n",
    "\n",
    "# init hidden state\n",
    "h = net.init_hidden(batch_size)\n",
    "\n",
    "net.eval()\n",
    "# iterate over test data\n",
    "for inputs, labels in test_loader:\n",
    "\n",
    "    # Creating new variables for the hidden state, otherwise\n",
    "    # we'd backprop through the entire training history\n",
    "    h = tuple([each.data for each in h])\n",
    "\n",
    "    if(train_on_gpu):\n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "    # get predicted outputs\n",
    "    output, h = net(inputs, h)\n",
    "    \n",
    "    # calculate loss\n",
    "    test_loss = criterion(output, labels)\n",
    "    test_losses.append(test_loss.item())\n",
    "    \n",
    "    # convert output probabilities to predicted class\n",
    "    _, pred = torch.max(output,1)\n",
    "    \n",
    "    # compare predictions to true label\n",
    "    correct_tensor = pred.eq(labels.view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())\n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "# -- stats! -- ##\n",
    "# avg test loss\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "\n",
    "# accuracy over all test data\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d61cc2aad4ddc8dafe532c04903c1c6931eb3564"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
